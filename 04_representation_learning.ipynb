{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "private_outputs": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sudimuk2017/AI-for-people-Workshop/blob/master/04_representation_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NzqxHIh4OCdW"
      },
      "source": [
        "# Incremental learning on image classification"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Today's practical session is an exploratory session designed to introduce you to some of the logging and visualisation techniques used by ML researchers to ensure that experiements are working well and for comparig new ideas to baselines, etc.\n",
        "\n",
        "The code comes from the [github repository](https://github.com/manuelemacchia/incremental-learning-image-classification) attached to the literary analysis paper of the same name which presents ideas in representation and incremental learning as of CVPR 2017.\n",
        "\n",
        "There will be very little coding today, so the emphasis will be on using a few tools to analyze whether or not new techniques are working and if they are, in fact, performing better than baseline.\n",
        "\n",
        "The tools we will explore are:\n",
        "- tensorboard: this package was devloped by google for tensorflow, though it works equally well with PyTroch. It allows users to log training data and watch the logs change during the training procedure. This package has been baked into the original notebook and will run during training so you can watch the loss and accuracy change over training.\n",
        "- [tSNE](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding): Similar to PCA, ICA, and AutoEncoders, this tool is used to reduce the dimmensionality of a set of inputs for better visualisation. Since the goal of incremental learning is to stop the model from forgetting what it has already learned, tSNE can be used to ensure that the representations of old classes remain similar after training. **<ins>You will need to find the three lines in each execution cell and comment them back in in order to enable tSNE,</ins>** but once you do, the full trianing round will create a png image of the 4 training splits, and we will ask you to analyze these plots for better understanding.\n",
        "- Confusion Matrices: These tools are incredibly valuable for showing classifier accuracy. The premise is to create a grid where the y dimmension represents the ground truth, and the x simmension represents predicted classes, and the values in each cell (apart from the diagonals) show misclassification (and the diagonals show correct clasifications). Using this plot, we can visualise the effect of catastrophic forgetting. **<ins>This code is included as well, but you will once again need to find where it is called in each executino block and uncomment it.</ins>**\n",
        "\n",
        "As mentioned, the coding for this task is quite straight forward, so we want you to come at this as if it is your own project and you are trying to make sure that the results of your experiment are correct.\n",
        "\n",
        "That and, of course, **<ins>we have snuck in a tiny bit of sabotage to the training of one of the methods, use the tools provided to determine where ths error is, and correct it using details from the other models.</ins>**\n",
        "\n",
        "**<ins>Keep an eye out for the insight questions asked in certain sections of the notebook - discuss them with your neighbors and see if you can convince them and yourself of the answers using both qualitative and quantitative information from the notebook.</ins>**"
      ],
      "metadata": {
        "id": "uev_a_X2JhrF"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wBHSznCZxpNB"
      },
      "source": [
        "## Libraries and packages\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xAYXtIdpx0Yy"
      },
      "source": [
        "import os\n",
        "import urllib\n",
        "import logging\n",
        "from importlib import reload\n",
        "import time\n",
        "\n",
        "import torch\n",
        "import torchsummary\n",
        "import torch.nn as nn\n",
        "import torch.nn.init as init\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, Subset, DataLoader, ConcatDataset\n",
        "from torch.backends import cudnn\n",
        "\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torchvision.models import resnet34\n",
        "\n",
        "from PIL import Image\n",
        "from copy import deepcopy\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%load_ext tensorboard\n",
        "\n",
        "from torch.utils.tensorboard import SummaryWriter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download packages from repository\n",
        "!git clone https://github.com/manuelemacchia/incremental-learning-image-classification.git\n",
        "!mv -v incremental-learning-image-classification/* .\n",
        "!rm -rf incremental-learning-image-classification README.md\n",
        "!gdown https://drive.google.com/drive/folders/10LKjtXO6ep8R2vQ6IG8B8Rb6mt94JS4N?usp=sharing -O /content/code_edits --folder\n",
        "!cp -f /content/code_edits/icarl.py /content/model/icarl.py\n",
        "!cp -f /content/code_edits/lwf.py /content/model/lwf.py\n",
        "!cp -f /content/code_edits/manager.py /content/model/manager.py\n",
        "!cp -f /content/code_edits/plot.py /content/utils/plot.py\n",
        "!cp -f /content/code_edits/tsne.py /content/tsne.py\n",
        "!rm -rf /content/code_edits"
      ],
      "metadata": {
        "id": "FPlafXI_W2ym"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "09iWc_oCotu2"
      },
      "source": [
        "# This code block imports the modules included in the git repo, which is why it has to come after the git extraction\n",
        "import model\n",
        "from data.cifar100 import Cifar100\n",
        "from model.resnet_cifar import resnet32\n",
        "from model.manager import Manager\n",
        "from model.lwf import LWF\n",
        "from model.icarl import Exemplars\n",
        "from model.icarl import iCaRL\n",
        "from utils import plot\n",
        "from tsne import FeatureTSNE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "metadata": {
        "id": "2FDkDHQdygqi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j12pgffMR6Qv"
      },
      "source": [
        "## Arguments\n",
        "\n",
        "These are constants, they will also not produce any output."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JwE0x8gkSisn"
      },
      "source": [
        "# Directories\n",
        "DATA_DIR = 'data'       # Directory where the dataset will be downloaded\n",
        "\n",
        "# Settings\n",
        "DEVICE = 'cuda'\n",
        "\n",
        "# Dataset\n",
        "#RANDOM_STATES = [658, 423, 422]      # For reproducibility of results\n",
        "RANDOM_STATES = [658]                # Note: different random states give very different\n",
        "                                     # splits and therefore very different results.\n",
        "\n",
        "NUM_CLASSES = 100       # Total number of classes\n",
        "\n",
        "VAL_SIZE = 0.1          # Proportion of validation set with respect to training set (between 0 and 1)\n",
        "\n",
        "# Training\n",
        "BATCH_SIZE = 64         # Batch size (iCaRL sets this to 128)\n",
        "LR = 0.1                 # Initial learning rate\n",
        "# Hmmmmm....?\n",
        "\n",
        "\n",
        "MOMENTUM = 0.9          # Momentum for stochastic gradient descent (SGD)\n",
        "WEIGHT_DECAY = 1e-5     # Weight decay from iCaRL\n",
        "\n",
        "NUM_RUNS = 1            # Number of runs of every method\n",
        "                        # Note: this should be at least 3 to have a fair benchmark\n",
        "\n",
        "NUM_EPOCHS = 35         # Total number of training epochs\n",
        "MILESTONES = [24, 31]   # Step down policy from iCaRL (MultiStepLR)\n",
        "                        # Decrease the learning rate by gamma at each milestone\n",
        "GAMMA = 0.2             # Gamma factor from iCaRL\n",
        "\n",
        "NUM_SPLITS = 4          # Number of splits (maximum is 10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJqnljCV5gJ5"
      },
      "source": [
        "## Fine tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HDDdumxRwbdQ"
      },
      "source": [
        "### Data preparation\n",
        "\n",
        "The next two blocks are used to instnatiate the several training and testing splits needed for the experiments. The training sets each contain ten different classes, while the test sets include every class from the current and previous splits."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "skknIP5Jwspm"
      },
      "source": [
        "# Define transformations for training\n",
        "train_transform = transforms.Compose([transforms.RandomCrop(32, padding=4),\n",
        "                                      transforms.RandomHorizontalFlip(),\n",
        "                                      transforms.ToTensor(), # Turn PIL Image to torch.Tensor\n",
        "                                      transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "# Define transformations for evaluation\n",
        "test_transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fdp8D_27twFn"
      },
      "source": [
        "train_dataloaders = [[] for i in range(NUM_RUNS)]\n",
        "val_dataloaders = [[] for i in range(NUM_RUNS)]\n",
        "test_dataloaders = [[] for i in range(NUM_RUNS)]\n",
        "\n",
        "for run_i in range(NUM_RUNS):\n",
        "    test_subsets = []\n",
        "    random_state = RANDOM_STATES[run_i]\n",
        "\n",
        "    for split_i in range(10):\n",
        "        # Download dataset only at first instantiation\n",
        "        if run_i+split_i == 0:\n",
        "            download = True\n",
        "        else:\n",
        "            download = False\n",
        "\n",
        "        # Create CIFAR100 dataset\n",
        "        train_dataset = Cifar100(DATA_DIR, train=True, download=download, random_state=random_state, transform=train_transform)\n",
        "        test_dataset = Cifar100(DATA_DIR, train=False, download=False, random_state=random_state, transform=test_transform)\n",
        "\n",
        "        # Subspace of CIFAR100 of 10 classes\n",
        "        train_dataset.set_classes_batch(train_dataset.batch_splits[split_i])\n",
        "        test_dataset.set_classes_batch([test_dataset.batch_splits[i] for i in range(0, split_i+1)])\n",
        "\n",
        "        # Define train and validation indices\n",
        "        train_indices, val_indices = train_dataset.train_val_split(VAL_SIZE, random_state)\n",
        "\n",
        "        train_dataloaders[run_i].append(DataLoader(Subset(train_dataset, train_indices),\n",
        "                                                   batch_size=BATCH_SIZE, shuffle=True, num_workers=2, drop_last=True))\n",
        "\n",
        "        val_dataloaders[run_i].append(DataLoader(Subset(train_dataset, val_indices),\n",
        "                                                 batch_size=BATCH_SIZE, shuffle=True, num_workers=2, drop_last=True))\n",
        "\n",
        "        # Dataset with all seen class\n",
        "        test_dataloaders[run_i].append(DataLoader(test_dataset,\n",
        "                                                  batch_size=BATCH_SIZE, shuffle=True, num_workers=2))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mNwcf1fpsvm_"
      },
      "source": [
        "# Sanity check: visualize a batch of images\n",
        "dataiter = test_dataloaders[0][0] # The first index controls which run (these vary by random seeds). The second index controls the split, play with this to see how we add classes per split.\n",
        "for images, labels in dataiter:\n",
        "    plot.image_grid(images, one_channel=False)\n",
        "    print(set(labels.tolist()))\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7EmP5GRP1JoP"
      },
      "source": [
        "### Execution\n",
        "\n",
        "To log the output the below box will be constantly printing the epochs and their stats above the tensorboard subwindow. Click the gear icon in the top right and check the \"Reload data\" box. As soon as the training starts the box below will show the training progress. If it doesn't show the traning progress, click the refresh button in the top right."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorboard --logdir=runs --reload_interval=30"
      ],
      "metadata": {
        "id": "CgbP-fApxvd5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This block actually trains and tests the model. The progress will be shown above. This loop is quite important.\n",
        "\n",
        "**TASK**: Try to understand what is happening and find the lines that are needed to output the tSNE and confusion matrix plots."
      ],
      "metadata": {
        "id": "oPeN4E0JAepP"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4JqA6VD_VxTg"
      },
      "source": [
        "logs = [[] for _ in range(NUM_RUNS)]\n",
        "for run_i in range(NUM_RUNS):\n",
        "    net = resnet32()\n",
        "\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "    featureTSNE_ft = FeatureTSNE('Fine_Tuning',DEVICE)\n",
        "    for split_i in range(NUM_SPLITS):\n",
        "        writer = SummaryWriter(f'runs/Fine_Tuning_{split_i}')\n",
        "        print(f\"## Split {split_i} of run {run_i} ##\")\n",
        "        start = time.time()\n",
        "        parameters_to_optimize = net.parameters()\n",
        "        optimizer = optim.SGD(parameters_to_optimize, lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
        "        scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=MILESTONES, gamma=GAMMA)\n",
        "\n",
        "        manager = Manager(DEVICE, net, criterion, optimizer, scheduler,\n",
        "                          train_dataloaders[run_i][split_i],\n",
        "                          val_dataloaders[run_i][split_i],\n",
        "                          test_dataloaders[run_i][split_i], writer)\n",
        "\n",
        "        scores = manager.train(NUM_EPOCHS)  # train the model\n",
        "\n",
        "        logs[run_i].append({})\n",
        "\n",
        "        # score[i] = dictionary with key:epoch, value: score\n",
        "        logs[run_i][split_i]['train_loss'] = scores[0]\n",
        "        logs[run_i][split_i]['train_accuracy'] = scores[1]\n",
        "        logs[run_i][split_i]['val_loss'] = scores[2]\n",
        "        logs[run_i][split_i]['val_accuracy'] = scores[3]\n",
        "\n",
        "        # Test the model on classes seen until now\n",
        "        test_accuracy, all_targets, all_preds = manager.test()\n",
        "\n",
        "        logs[run_i][split_i]['test_accuracy'] = test_accuracy\n",
        "        logs[run_i][split_i]['conf_mat'] = confusion_matrix(all_targets.to('cpu'), all_preds.to('cpu'))\n",
        "\n",
        "        featureTSNE_ft.get_feature_reps(manager.test_dataloader,manager.net,split_i)\n",
        "\n",
        "        print(f'Confusion Matrix: Split {split_i}')\n",
        "        plot.heatmap_cm(all_targets.cpu(), all_preds.cpu()) #the confusion matrix\n",
        "\n",
        "        # Add 10 nodes to last FC layer\n",
        "        manager.increment_classes(n=10)\n",
        "        print('Total Split Duration: ', time.time() - start)\n",
        "    featureTSNE_ft.plot_tsne_grids() ## For all tSNE plots"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**INSIGHT:**  The training and validation accuracy seem to go up after each split, why is the model getting better even though it's seeing new classes?\n",
        "\n",
        "**INSIGHT:** How do the confusion matrices after split 0 prove that there is catastrophic forgetting?\n",
        "\n",
        "**INSIGHT:** What do the tSNE plots for Fine Tuning imply about the feature representations learned by the model? (Use the colours of the points to find where old class representations end up in newer splits)"
      ],
      "metadata": {
        "id": "alhsvDEROa4o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plot"
      ],
      "metadata": {
        "id": "_Deici_f2SVX"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ESzk5gF2c_c"
      },
      "source": [
        "# This block simply collates and prepares the training and validation logs (separate ones form tensorboards) for plotting.\n",
        "train_loss = [[logs[run_i][i]['train_loss'] for i in range(NUM_SPLITS)] for run_i in range(NUM_RUNS)]\n",
        "train_accuracy = [[logs[run_i][i]['train_accuracy'] for i in range(NUM_SPLITS)] for run_i in range(NUM_RUNS)]\n",
        "val_loss = [[logs[run_i][i]['val_loss'] for i in range(NUM_SPLITS)] for run_i in range(NUM_RUNS)]\n",
        "val_accuracy = [[logs[run_i][i]['val_accuracy'] for i in range(NUM_SPLITS)] for run_i in range(NUM_RUNS)]\n",
        "test_accuracy = [[logs[run_i][i]['test_accuracy'] for i in range(NUM_SPLITS)] for run_i in range(NUM_RUNS)]\n",
        "\n",
        "train_loss = np.array(train_loss)\n",
        "train_accuracy = np.array(train_accuracy)\n",
        "val_loss = np.array(val_loss)\n",
        "val_accuracy = np.array(val_accuracy)\n",
        "test_accuracy = np.array(test_accuracy)\n",
        "\n",
        "train_loss_stats = np.array([train_loss.mean(0), train_loss.std(0)]).transpose()\n",
        "train_accuracy_stats = np.array([train_accuracy.mean(0), train_accuracy.std(0)]).transpose()\n",
        "val_loss_stats = np.array([val_loss.mean(0), val_loss.std(0)]).transpose()\n",
        "val_accuracy_stats = np.array([val_accuracy.mean(0), val_accuracy.std(0)]).transpose()\n",
        "test_accuracy_stats = np.array([test_accuracy.mean(0), test_accuracy.std(0)]).transpose()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And these next two plot the train/val and test stats"
      ],
      "metadata": {
        "id": "9R9A-KWHnpbh"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7TAdCtXDEa5d"
      },
      "source": [
        "plot.train_val_scores(train_loss_stats, train_accuracy_stats, val_loss_stats, val_accuracy_stats)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E1APlRtTpmkK"
      },
      "source": [
        "plot.test_scores(test_accuracy_stats)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bKJ_Z48QmQ2C"
      },
      "source": [
        "## Learning Without Forgetting\n",
        "\n",
        "The code following this uses the same structure as in the Fine Tuning. Data prep will produce no output, ensure the necessary lines are included in the execution block, watch the training carefully, and plot all the stats, confusion matrix, and tSNE plots."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHqtSdwzm16h"
      },
      "source": [
        "### Data preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "373M_sOAm16i"
      },
      "source": [
        "# Transformations for Learning Without Forgetting\n",
        "train_transform = transforms.Compose([transforms.RandomCrop(32, padding=4),\n",
        "                                      transforms.RandomHorizontalFlip(),\n",
        "                                      transforms.ToTensor(), # Turn PIL Image to torch.Tensor\n",
        "                                      transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97Gi3Sp8m16k"
      },
      "source": [
        "train_dataloaders = [[] for i in range(NUM_RUNS)]\n",
        "val_dataloaders = [[] for i in range(NUM_RUNS)]\n",
        "test_dataloaders = [[] for i in range(NUM_RUNS)]\n",
        "\n",
        "for run_i in range(NUM_RUNS):\n",
        "    test_subsets = []\n",
        "    random_state = RANDOM_STATES[run_i]\n",
        "\n",
        "    for split_i in range(NUM_SPLITS):\n",
        "        # Download dataset only at first instantiation\n",
        "        if run_i+split_i == 0:\n",
        "            download = True\n",
        "        else:\n",
        "            download = False\n",
        "\n",
        "        # Create CIFAR100 dataset\n",
        "        train_dataset = Cifar100(DATA_DIR, train=True, download=download, random_state=random_state, transform=train_transform)\n",
        "        test_dataset = Cifar100(DATA_DIR, train=False, download=False, random_state=random_state, transform=test_transform)\n",
        "\n",
        "        # Subspace of CIFAR100 of 10 classes\n",
        "        train_dataset.set_classes_batch(train_dataset.batch_splits[split_i])\n",
        "        test_dataset.set_classes_batch([test_dataset.batch_splits[i] for i in range(0, split_i+1)])\n",
        "\n",
        "        # Define train and validation indices\n",
        "        train_indices, val_indices = train_dataset.train_val_split(VAL_SIZE, random_state)\n",
        "\n",
        "        train_dataloaders[run_i].append(DataLoader(Subset(train_dataset, train_indices),\n",
        "                                                   batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True))\n",
        "\n",
        "        val_dataloaders[run_i].append(DataLoader(Subset(train_dataset, val_indices),\n",
        "                                                 batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True))\n",
        "\n",
        "        # Dataset with all seen class\n",
        "        test_dataloaders[run_i].append(DataLoader(test_dataset,\n",
        "                                                  batch_size=BATCH_SIZE, shuffle=True, num_workers=4))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ksaz2qZ5m16n"
      },
      "source": [
        "# Sanity check: visualize a batch of images\n",
        "dataiter = test_dataloaders[0][0] # The first index controls which run (these vary by random seeds). The second index controls the split, play with this to see how we add classes per split.\n",
        "for images, labels in dataiter:\n",
        "    plot.image_grid(images, one_channel=False)\n",
        "    print(set(labels.tolist()))\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iYwMtMJuLyYe"
      },
      "source": [
        "### Execution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JpGuC_hSL0jN"
      },
      "source": [
        "# Arguments for Learning without Forgetting\n",
        "BATCH_SIZE = 128\n",
        "LR = 2\n",
        "%tensorboard --logdir=runs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MlThDLCvXJwS"
      },
      "source": [
        "logs = [[] for _ in range(NUM_RUNS)]\n",
        "# Iterate over runs\n",
        "for run_i in range(NUM_RUNS):\n",
        "    net = resnet32()\n",
        "\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "    #featureTSNE_lwf = FeatureTSNE('LWF',DEVICE)\n",
        "\n",
        "    for split_i in range(NUM_SPLITS):\n",
        "        print(f\"## Split {split_i} of run {run_i} ##\")\n",
        "        writer = SummaryWriter(f'runs/LWF_{split_i}')\n",
        "        start = time.time()\n",
        "        # Redefine optimizer at each split (pass by reference issue)\n",
        "        parameters_to_optimize = net.parameters()\n",
        "        optimizer = optim.SGD(parameters_to_optimize, lr=LR,\n",
        "                                momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
        "        scheduler = optim.lr_scheduler.MultiStepLR(optimizer,\n",
        "                                                    milestones=MILESTONES, gamma=GAMMA)\n",
        "\n",
        "        num_classes = 10*(split_i+1)\n",
        "\n",
        "        if num_classes == 10: # old network == None\n",
        "            lwf = LWF(DEVICE, net, None, criterion, optimizer, scheduler,\n",
        "                            train_dataloaders[run_i][split_i],\n",
        "                            val_dataloaders[run_i][split_i],\n",
        "                            test_dataloaders[run_i][split_i],\n",
        "                            num_classes,writer)\n",
        "        else:\n",
        "            lwf = LWF(DEVICE, net, old_net, criterion, optimizer, scheduler,\n",
        "                            train_dataloaders[run_i][split_i],\n",
        "                            val_dataloaders[run_i][split_i],\n",
        "                            test_dataloaders[run_i][split_i],\n",
        "                            num_classes,writer)\n",
        "\n",
        "        scores = lwf.train(NUM_EPOCHS)  # train the model\n",
        "\n",
        "        logs[run_i].append({})\n",
        "\n",
        "        # score[i] = dictionary with key:epoch, value: score\n",
        "        logs[run_i][split_i]['train_loss'] = scores[0]\n",
        "        logs[run_i][split_i]['train_accuracy'] = scores[1]\n",
        "        logs[run_i][split_i]['val_loss'] = scores[2]\n",
        "        logs[run_i][split_i]['val_accuracy'] = scores[3]\n",
        "\n",
        "        # Test the model on classes seen until now\n",
        "        test_accuracy, all_targets, all_preds = lwf.test()\n",
        "        #featureTSNE_lwf.get_feature_reps(lwf.test_dataloader,lwf.net,split_i)\n",
        "\n",
        "        logs[run_i][split_i]['test_accuracy'] = test_accuracy\n",
        "        logs[run_i][split_i]['conf_mat'] = confusion_matrix(all_targets.to('cpu'), all_preds.to('cpu'))\n",
        "\n",
        "        print(f'Confusion Matrix: Split {split_i}')\n",
        "        plot.heatmap_cm(all_targets.cpu(), all_preds.cpu())\n",
        "\n",
        "        old_net = deepcopy(lwf.net)\n",
        "\n",
        "        lwf.increment_classes()\n",
        "        print('Total Split Duration: ', time.time() - start)\n",
        "    featureTSNE_lwf.plot_tsne_grids()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**INSIGHT:** The confusion matrices for LWF look much better, but there is still a problem. What happens as we see more and more classes?\n",
        "\n",
        "**INSIGHT:** The tSNE plots for LWF show a slightly different modification from split to split than the Fine Tuning plots do, what does this new insight imply about the feature representations? (bare in mind that feature represenations for similar classes would be similar if we used all the data)"
      ],
      "metadata": {
        "id": "X2kzvkY7Xp_Z"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2xZbK6EGSaZN"
      },
      "source": [
        "### Plots"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bUfgSq1xSbrD"
      },
      "source": [
        "train_loss = [[logs[run_i][i]['train_loss'] for i in range(NUM_SPLITS)] for run_i in range(NUM_RUNS)]\n",
        "train_accuracy = [[logs[run_i][i]['train_accuracy'] for i in range(NUM_SPLITS)] for run_i in range(NUM_RUNS)]\n",
        "val_loss = [[logs[run_i][i]['val_loss'] for i in range(NUM_SPLITS)] for run_i in range(NUM_RUNS)]\n",
        "val_accuracy = [[logs[run_i][i]['val_accuracy'] for i in range(NUM_SPLITS)] for run_i in range(NUM_RUNS)]\n",
        "test_accuracy = [[logs[run_i][i]['test_accuracy'] for i in range(NUM_SPLITS)] for run_i in range(NUM_RUNS)]\n",
        "\n",
        "train_loss = np.array(train_loss)\n",
        "train_accuracy = np.array(train_accuracy)\n",
        "val_loss = np.array(val_loss)\n",
        "val_accuracy = np.array(val_accuracy)\n",
        "test_accuracy = np.array(test_accuracy)\n",
        "\n",
        "train_loss_stats = np.array([train_loss.mean(0), train_loss.std(0)]).transpose()\n",
        "train_accuracy_stats = np.array([train_accuracy.mean(0), train_accuracy.std(0)]).transpose()\n",
        "val_loss_stats = np.array([val_loss.mean(0), val_loss.std(0)]).transpose()\n",
        "val_accuracy_stats = np.array([val_accuracy.mean(0), val_accuracy.std(0)]).transpose()\n",
        "test_accuracy_stats = np.array([test_accuracy.mean(0), test_accuracy.std(0)]).transpose()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1w3_YPJCSeli"
      },
      "source": [
        "plot.train_val_scores(train_loss_stats, train_accuracy_stats, val_loss_stats, val_accuracy_stats)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZSt6-FJbSelp"
      },
      "source": [
        "plot.test_scores(test_accuracy_stats)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7B6CcDcDlMf_"
      },
      "source": [
        "## iCaRL\n",
        "\n",
        "The code following this uses the same structure as in the Fine Tuning and LWF. Data prep will produce no output, ensure the necessary lines are included in the execution block, watch the training carefully, and plot all the stats, confusion matrix, and tSNE plots."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rF0ypxGognNR"
      },
      "source": [
        "### Data preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4mf04UjEgmPG"
      },
      "source": [
        "# Transformations for Learning Without Forgetting\n",
        "train_transform = transforms.Compose([transforms.RandomCrop(32, padding=4),\n",
        "                                      transforms.RandomHorizontalFlip(),\n",
        "                                      transforms.ToTensor(), # Turn PIL Image to torch.Tensor\n",
        "                                      transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y9Oq44dxgmPN"
      },
      "source": [
        "train_subsets = [[] for i in range(NUM_RUNS)]\n",
        "val_subsets = [[] for i in range(NUM_RUNS)]\n",
        "test_subsets = [[] for i in range(NUM_RUNS)]\n",
        "\n",
        "for run_i in range(NUM_RUNS):\n",
        "    for split_i in range(10):\n",
        "        if run_i+split_i == 0: # Download dataset only at first instantiation\n",
        "            download = True\n",
        "        else:\n",
        "            download = False\n",
        "\n",
        "        # Create CIFAR100 dataset\n",
        "        train_dataset = Cifar100(DATA_DIR, train=True, download=download, random_state=RANDOM_STATES[run_i], transform=train_transform)\n",
        "        test_dataset = Cifar100(DATA_DIR, train=False, download=False, random_state=RANDOM_STATES[run_i], transform=test_transform)\n",
        "\n",
        "        # Subspace of CIFAR100 of 10 classes\n",
        "        train_dataset.set_classes_batch(train_dataset.batch_splits[split_i])\n",
        "        test_dataset.set_classes_batch([test_dataset.batch_splits[i] for i in range(0, split_i+1)])\n",
        "\n",
        "        # Define train and validation indices\n",
        "        train_indices, val_indices = train_dataset.train_val_split(VAL_SIZE, RANDOM_STATES[run_i])\n",
        "\n",
        "        # Define subsets\n",
        "        train_subsets[run_i].append(Subset(train_dataset, train_indices))\n",
        "        val_subsets[run_i].append(Subset(train_dataset, val_indices))\n",
        "        test_subsets[run_i].append(test_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cmCpRMBKgvDB"
      },
      "source": [
        "### Execution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-nRU--zYjmZT"
      },
      "source": [
        "# iCaRL hyperparameters\n",
        "LR = 2\n",
        "MOMENTUM = 0.9\n",
        "WEIGHT_DECAY = 0.00001\n",
        "MILESTONES = [24, 31]\n",
        "GAMMA = 0.2\n",
        "NUM_EPOCHS = 35\n",
        "BATCH_SIZE = 64\n",
        "%tensorboard --logdir=runs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1mcTQUN7VLPF"
      },
      "source": [
        "# Define what tests to run\n",
        "TEST_ICARL = True # Run test with iCaRL (exemplars + train dataset)\n",
        "TEST_HYBRID1 = False # Run test with hybrid1\n",
        "\n",
        "# Initialize logs\n",
        "logs_icarl = [[] for _ in range(NUM_RUNS)]\n",
        "logs_hybrid1 = [[] for _ in range(NUM_RUNS)]\n",
        "for run_i in range(NUM_RUNS):\n",
        "    net = resnet32()\n",
        "    icarl = iCaRL(DEVICE, net, LR, MOMENTUM, WEIGHT_DECAY, MILESTONES, GAMMA, NUM_EPOCHS, BATCH_SIZE, train_transform, test_transform)\n",
        "    featureTSNE_ic = FeatureTSNE('iCARL',DEVICE)\n",
        "    for split_i in range(NUM_SPLITS):\n",
        "        print(f\"## Split {split_i} of run {run_i} ##\")\n",
        "        writer = SummaryWriter(f'runs/iCaRL_{split_i}')\n",
        "        start = time.time()\n",
        "        icarl.writer = writer\n",
        "        icarl.train_step = 0\n",
        "        train_logs = icarl.incremental_train(split_i, train_subsets[run_i][split_i], val_subsets[run_i][split_i])\n",
        "\n",
        "        if TEST_ICARL:\n",
        "            logs_icarl[run_i].append({})\n",
        "\n",
        "            acc, all_targets, all_preds = icarl.test(test_subsets[run_i][split_i], train_subsets[run_i][split_i])\n",
        "\n",
        "            print(f'Confusion Matrix: Split {split_i}')\n",
        "            plot.heatmap_cm(all_targets.cpu(), all_preds.cpu())\n",
        "\n",
        "            featureTSNE_ic.get_feature_reps(icarl.test_dataloader,icarl.net,split_i)\n",
        "\n",
        "            logs_icarl[run_i][split_i]['accuracy'] = acc\n",
        "            logs_icarl[run_i][split_i]['conf_mat'] = confusion_matrix(all_targets.to('cpu'), all_preds.to('cpu'))\n",
        "\n",
        "            logs_icarl[run_i][split_i]['train_loss'] = train_logs[0]\n",
        "            logs_icarl[run_i][split_i]['train_accuracy'] = train_logs[1]\n",
        "            logs_icarl[run_i][split_i]['val_loss'] = train_logs[2]\n",
        "            logs_icarl[run_i][split_i]['val_accuracy'] = train_logs[3]\n",
        "\n",
        "        if TEST_HYBRID1:\n",
        "            logs_hybrid1[run_i].append({})\n",
        "\n",
        "            acc, all_targets, all_preds = icarl.test_without_classifier(test_subsets[run_i][split_i])\n",
        "\n",
        "            logs_hybrid1[run_i][split_i]['accuracy'] = acc\n",
        "            logs_hybrid1[run_i][split_i]['conf_mat'] = confusion_matrix(all_targets.to('cpu'), all_preds.to('cpu'))\n",
        "        print('Total Split Duration: ', time.time() - start)\n",
        "    featureTSNE_ic.plot_tsne_grids()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define what tests to run\n",
        "TEST_ICARL = True # Run test with iCaRL (exemplars + train dataset)\n",
        "TEST_HYBRID1 = False # Run test with hybrid1\n",
        "\n",
        "# Initialize logs\n",
        "logs_icarl = [[] for _ in range(NUM_RUNS)]\n",
        "logs_hybrid1 = [[] for _ in range(NUM_RUNS)]\n",
        "for run_i in range(NUM_RUNS):\n",
        "    net = resnet32()\n",
        "    icarl = iCaRL(DEVICE, net, LR, MOMENTUM, WEIGHT_DECAY, MILESTONES, GAMMA, NUM_EPOCHS, BATCH_SIZE, train_transform, test_transform)\n",
        "    featureTSNE_ic = FeatureTSNE('iCARL',DEVICE)\n",
        "    for split_i in range(NUM_SPLITS):\n",
        "        print(f\"## Split {split_i} of run {run_i} ##\")\n",
        "        writer = SummaryWriter(f'runs/iCaRL_{split_i}')\n",
        "        start = time.time()\n",
        "        icarl.writer = writer\n",
        "        icarl.train_step = 0\n",
        "        train_logs = icarl.incremental_train(split_i, train_subsets[run_i][split_i], val_subsets[run_i][split_i])\n",
        "\n",
        "        if TEST_ICARL:\n",
        "            logs_icarl[run_i].append({})\n",
        "\n",
        "            acc, all_targets, all_preds = icarl.test(test_subsets[run_i][split_i], train_subsets[run_i][split_i])\n",
        "\n",
        "            print(f'Confusion Matrix: Split {split_i}')\n",
        "            plot.heatmap_cm(all_targets.cpu(), all_preds.cpu())\n",
        "\n",
        "            featureTSNE_ic.get_feature_reps(icarl.test_dataloader,icarl.net,split_i)\n",
        "\n",
        "            logs_icarl[run_i][split_i]['accuracy'] = acc\n",
        "            logs_icarl[run_i][split_i]['conf_mat'] = confusion_matrix(all_targets.to('cpu'), all_preds.to('cpu'))\n",
        "\n",
        "            logs_icarl[run_i][split_i]['train_loss'] = train_logs[0]\n",
        "            logs_icarl[run_i][split_i]['train_accuracy'] = train_logs[1]\n",
        "            logs_icarl[run_i][split_i]['val_loss'] = train_logs[2]\n",
        "            logs_icarl[run_i][split_i]['val_accuracy'] = train_logs[3]\n",
        "\n",
        "        if TEST_HYBRID1:\n",
        "            logs_hybrid1[run_i].append({})\n",
        "\n",
        "            acc, all_targets, all_preds = icarl.test_without_classifier(test_subsets[run_i][split_i])\n",
        "\n",
        "            logs_hybrid1[run_i][split_i]['accuracy'] = acc\n",
        "            logs_hybrid1[run_i][split_i]['conf_mat'] = confusion_matrix(all_targets.to('cpu'), all_preds.to('cpu'))\n",
        "        print('Total Split Duration: ', time.time() - start)\n",
        "    featureTSNE_ic.plot_tsne_grids()"
      ],
      "metadata": {
        "id": "s9nZbwe5J5ol"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**INSIGHT:** iCaRL produces very high quality confusion matrices, but what could be done to improve them even more? (this is not a trick question, just remember that we are working with very limited resources)\n",
        "\n",
        "**INSIGHT:** The tSNE plots also show a lot of a promise for iCaRL. Looking particularly at the plot for split 3, what characteristic of the plot shows us the feature representations are high quality?\n",
        "\n",
        "**INSIGHT:** The training process for iCaRL and LWF have an additional loss term called disillation loss. How is this additional loss term reflected in the training and validation curves?"
      ],
      "metadata": {
        "id": "6RUS6ye0YYUL"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wuDM6Ydokv05"
      },
      "source": [
        "### Plots"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nEn9P6usbwYa"
      },
      "source": [
        "train_loss = [[logs_icarl[run_i][i]['train_loss'] for i in range(NUM_SPLITS)] for run_i in range(NUM_RUNS)]\n",
        "train_accuracy = [[logs_icarl[run_i][i]['train_accuracy'] for i in range(NUM_SPLITS)] for run_i in range(NUM_RUNS)]\n",
        "val_loss = [[logs_icarl[run_i][i]['val_loss'] for i in range(NUM_SPLITS)] for run_i in range(NUM_RUNS)]\n",
        "val_accuracy = [[logs_icarl[run_i][i]['val_accuracy'] for i in range(NUM_SPLITS)] for run_i in range(NUM_RUNS)]\n",
        "test_accuracy = [[logs_icarl[run_i][i]['accuracy'] for i in range(NUM_SPLITS)] for run_i in range(NUM_RUNS)]\n",
        "\n",
        "train_loss = np.array(train_loss)\n",
        "train_accuracy = np.array(train_accuracy)\n",
        "val_loss = np.array(val_loss)\n",
        "val_accuracy = np.array(val_accuracy)\n",
        "test_accuracy = np.array(test_accuracy)\n",
        "\n",
        "train_loss_stats = np.array([train_loss.mean(0), train_loss.std(0)]).transpose()\n",
        "train_accuracy_stats = np.array([train_accuracy.mean(0), train_accuracy.std(0)]).transpose()\n",
        "val_loss_stats = np.array([val_loss.mean(0), val_loss.std(0)]).transpose()\n",
        "val_accuracy_stats = np.array([val_accuracy.mean(0), val_accuracy.std(0)]).transpose()\n",
        "test_accuracy_stats = np.array([test_accuracy.mean(0), test_accuracy.std(0)]).transpose()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-GmiU4BfhVht"
      },
      "source": [
        "plot.train_val_scores(train_loss_stats, train_accuracy_stats, val_loss_stats, val_accuracy_stats)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PAWPfif3j7pK"
      },
      "source": [
        "plot.test_scores(test_accuracy_stats)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ozvTec_bHznl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}